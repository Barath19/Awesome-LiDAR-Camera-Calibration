# Awesome-LiDAR-Camera-Calibration

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A Collection of LiDAR-Camera-Calibration Papers, Toolboxes and Notes. 

**Outline**
- [Introduction](#0-introduction)
- [Target-based methods](#1-target-based-methods)
- [Targetless methods](#2-targetless-methods)
  - [motion-based methods](#21-motion-based-methods)
  - [scene-based-methods](#22-scene-based-methods)
- [Other Toolboxes](#3-other-toolboxes)

## 0. Introduction
For applications such as autonomous driving, robotics, navigation systems, and 3-D scene reconstruction, data of the same scene is often captured using both lidar and camera sensors. To accurately interpret the objects in a scene, it is necessary to fuse the lidar and the camera outputs together. Lidar camera calibration estimates a **rigid transformation matrix (extrinsics, rotation+translation, 6 DoF)** that establishes the correspondences between the points in the 3-D lidar plane and the pixels in the image plane. 

![Example](...)
## 1. Target-based methods


## 2. Targetless methods

### 2.1 motion-based methods

### 2.2 scene-based methods


## 3. Other toolboxes
